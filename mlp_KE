#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
@author: krajna4ever
"""

#import all the libraries
import os
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import math
import tensorflow as tf
import numpy as np 
import pandas as pd
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.utils import np_utils
from pandas import read_csv
from pandas import DataFrame
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from keras.layers import BatchNormalization
from keras.callbacks import EarlyStopping
from keras import optimizers 
from keras.layers import LSTM, Input
from keras.models import Model 
from sklearn.model_selection import KFold
from keras import backend as K
import keras
import math
from keras import models, layers, optimizers, callbacks 
from keras.utils.vis_utils import plot_model


#impor the dataset
np.random.seed(7)
data=pd.read_csv('beacons.csv', engine='python')
data.head(5)

def l2_dist(p1,p2):
    x1,y1 = p1
    x2,y2 = p2
    x1,y1 = np.array(x1), np.array(y1)
    x2,y2 = np.array(x2), np.array(y2)
    dx = x1-x2
    dy = y1-y2
    dx = dx**2
    dy = dy**2
    dists = dx+dy
    dists = np.sqrt(dists)
    return np.mean(dists), dists
#allocate bbeacon values 
beacon_values = data.iloc[:,3:]
print(beacon_values)

beacon_values = data.iloc [:,3:].values
print(data.shape)
X = data.iloc[:,3:]

scaler = MinMaxScaler(feature_range=(0,1))
X_scaler = scaler.fit_transform(X)
Y=data.iloc[:,1:3]
# Y=pd.DataFrame(Y).to_numpy()
print(Y)


coordinates = data.iloc[:,1:3].values
print(coordinates)
X_train, X_test, Y_train, Y_test = train_test_split(X, coordinates, test_size=0.2, random_state=42)

print(X_train.shape)
print(Y_train.shape)
    #start the structure 
model = Sequential()
model.add(Dense(1000, input_dim=7, activation='tanh'))
model.add(BatchNormalization())
model.add(Dense(1000, activation='tanh'))
model.add(Dense(1000, activation='tanh'))
model.add(Dense(1000, activation='tanh'))
model.add(Dense(500, activation='tanh'))
model.add(Dense(2, activation='relu'))
model.summary()
adam = optimizers.Adam(lr=0.002, beta_1=0.9, beta_2=0.999, amsgrad=False)
rms = optimizers.RMSprop(lr=0.00001,rho=0.9)
model.compile(loss='mse', optimizer=rms, metrics=['accuracy'])
es = EarlyStopping(monitor='val_accuracy', patience=5, verbose=1, mode='auto', restore_best_weights=True,)
chkpt = callbacks.ModelCheckpoint(filepath='moja_modulara_{val_acc:.2f}.h5', monitor='mse',verbose=1,save_best_only=True, save_weights_only=False)
reduce_lr=callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.95,patience=5,verbose=1,cooldown=2)
callb_l=[chkpt, reduce_lr]


#train
hist = model.fit(X_train, y=Y_train, validation_data=(X_test, Y_test), epochs=2, batch_size=12, verbose=1 , callbacks=callb_l)
model.save(filepath=r'mojamoduara.h5', overwrite=True)

plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train','test'], loc='upper left')
plt.show()


loss_train = hist.history['acc']
loss_val = hist.history['val_acc']
epochs = range(1,201)
plt.plot(epochs, loss_train, 'g', label='Training accuracy')
plt.plot(epochs, loss_val, 'b', label='validation accuracy')
plt.title('Training and Validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()




preds=model.predict(X_test)
print(preds)
cv=KFold(n_splits=3, random_state=42, shuffle=True)
for train_index, test_index in cv.split(X):
    print("Train Index", train_index, "\n")
    print("Test Index: ", test_index)
    
    X_train, X_test, Y_train, Y_test = X.iloc[train_index], X.iloc[test_index], Y.iloc[train_index], Y.iloc[test_index]
    model=Sequential()
    model.add(Dense(100, input_dim=7, activation='tanh'))
    model.add(BatchNormalization())
    model.add(Dense(50, activation='tanh'))
    model.add(Dense(2, activation='relu'))
    X_train, X_test, Y_train, Y_test=X.iloc[train_index], X.iloc[test_index], Y.iloc[train_index], Y.iloc[test_index]
    model.compile(loss=custom_loss, optimizer=rms)
    model.fit(X_train, Y_train, epochs=150, batch_size=32, validation_split=0.2, verbose=2)
    nov=model.predict(X_test)
joj=pd.DataFrame(preds)
joj = joj.rename(columns={0:'a',1:'b'})
    
print(joj)
    
aktual=pd.DataFrame(np.array(Y_test).T)
aktual_1= aktual.T
aktual_1 = aktual.rename(columns={0:'x',1:'y'})
    
print(aktual_1)
def eucledian_distance(p1, p2):
    x1,y1 = p1
    x2,y2 = p2
    x1, y1 = np.array(x1), np.array(y1)
    x2, y2 = np.array(x2), np.array(y2)
    dx = x1 - x2
    dy = y1 - y2
    dists = np.sqrt(dx ** 2 + dy ** 2)
    return np.mean(dists), dists

Ypred=model.predict(X_test)
Xcoor = Y_test[:,0]
Ycoor = Y_test[:,1]
PredXcoor = Ypred[:,0]
PredYcoor = Ypred[:,1]
# PredXcoor = joj.iloc[:,0]
l2dists_mean, l2dists = eucledian_distance((PredXcoor, PredYcoor), (Xcoor, Ycoor))
print("Mean distance error:{}" .format(l2dists_mean))

predictions = PredXcoor
predictions2 = PredYcoor
errors = abs(predictions - Xcoor)
errors2 = abs(predictions2-Ycoor)
mape = 100 * (errors / PredXcoor)
accuracy = 100 - np.mean(mape)


print('Mean Absolute Error:', round(np.mean(errors), 2), 'm')
print('Mean Absolute Error1:', round(np.mean(errors2), 2), 'm')

import os
cdf_error_cnn = os.path.join(os.getcwd(),'CDF_error_mlpara.png')
sortedl2_deep = np.sort(l2dists)
prob_deep = 1. * np.arange(len(sortedl2_deep))/(len(sortedl2_deep) - 1)
fig, ax = plt.subplots()
lg1, = ax.plot(sortedl2_deep, prob_deep, color='black')
plt.title('CDF of Euclidean distance error for MLP')
plt.xlabel('Distance (m)')
plt.ylabel('Probability')
plt.grid(True)
gridlines = ax.get_xgridlines() + ax.get_ygridlines()
for line in gridlines:
            line.set_linestyle('-.')

plt.savefig(cdf_error_cnn, dpi=300)
plt.show()
 
